{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h6>HMM</h6>\n",
    "This is the best derivation bc it is so simple. Consistent notation. \n",
    "simplified version of RV Z=hidden states and x is observed. start with joint probability of observed given \n",
    "hidden and factor out. \n",
    "<img src=\"images/hmm1_1.png\">\n",
    "2 sets of parameters, transision prob and emission prob. \n",
    "<p></p>\n",
    "1)transition probabiliies, from hidden state to next hidden state assign real number probabilities and total \n",
    "law of probability all j Pr have to sum to 1. N by M matrix Tij and is stochastic if it satisfies TP law. M is number\n",
    "of hidden states and N is number of observables. \n",
    "<p></p>\n",
    "2) emission probabilities from S to output. \n",
    "3) initial distribution vector. \n",
    "<img src=\"images/hmm1_2.png\">\n",
    "write the joint distribtion in terms of parameters, multiply by all terms. Only some allowed states. \n",
    "Verify the coursera is MxN and only some are allowed. \n",
    "<img src=\"images/hmm1_3.png\">\n",
    "<img src=\"images/hmm1_4.png\">\n",
    "forward and backward HMM to do inference. Or to select path through graph network of MxN size. What is the \n",
    "dynamic programming part? The lookup into the T and E probability matrices? The FB model assumes know initial distribution, \n",
    "transition probability, emission probability. FB compute p(zk|xn); this computes the probability of hidden states\n",
    "given the observables. This is computing marginal probability given x..xn observations. break apart to foward and backwardd\n",
    "part. Forward is joint proability p(zk|x1..k) and backward p(x1..k|zk). So now we have forward and backward but no\n",
    "way to calculate them. Lets look at the probability of Zk, the hidden states. This is proportional to the probability\n",
    "of the observed states and hidden states. \n",
    "<img src=\"images/hmm1_5.png\">\n",
    "how to we calculate the conditional probability? we used conditional independence to separate. First we know the \n",
    "probability of Zk(x) is proportional to P(zk,x) = P(xk+1:N,|zk,x1..xk)P(zk,x1,..k). The conditional independence part\n",
    "allows us to get rid of teh x1,,k part. Teh conditional independence part you can see w/hmm chain becuase we are blocked\n",
    "at the tails. they are seprated, x1, x2 are conditioanlly independent of x3,..xn given z2. So we can get rid of the x1,N part. \n",
    "\n",
    "<img src=\"images/hmm1_6.png\">\n",
    "so what we did does that help? Whoa yes this is now the backward part. And the other term is the forward part!!! \n",
    "the porportional part means you have to add a normalizing constant over a finite set. The key part to getting\n",
    "the posterior distru=ibution on teh hidden states given the xs is to decompose to forward and backward steps. \n",
    "<img src=\"images/hmm1_7.png\">\n",
    "You can do change detection. You can estimate parameters using Baum welch which couples foward backward with EM.\n",
    "after you complete forward backward you can sample posterior you can sample zs given xs. You can visualize if given \n",
    "data like handwritten letters, and want to sample from possible interpretations. You can get most likely example which is \n",
    "viterbi. YOu can see what most likely samples are. how to compute these using forward and backward. \n",
    "<img src=\"images/hmm1_8.png\">\n",
    "using forward algorithm for HMM example of dynamic programming. There is one trick for forward algorithm. \n",
    "Goal is want to compute the joint distribtion on zk and x1..k, p(zk,x1:k) for all k and zk. We are given x. \n",
    "We have transition and emission probabilities, how to get p(zk,x1:k)? try summing over zk first and then factoring to \n",
    "using the data we have. how to factor? we have xk given zk, zk-1, x1:k-1. Conditional independec.e We get transition\n",
    "    probability. The trick is to move back or forwad one step and factor and you get a recursion for hting you \n",
    "    are trying to compute. \n",
    "<img src=\"images/hmm1_9.png\">\n",
    "Applying markov propeties. Claim xk is conditionally independent of zk, zk-1, xk-1. If we condition on zk then xk\n",
    "is indenpent of the chain before it where the previous chain is defined as zk-1 and xk-1. Because they are condintionally\n",
    "indepenent we can block and simplify. \n",
    "<img src=\"images/hmm1_10.png\">\n",
    "how about next term? We should be able to propagate the conditional independence markov assumption. \n",
    "<img src=\"images/hmm1_11.png\">\n",
    "first part is emission prob, second part transition prob, last part is function of last state, x we know zk is \n",
    "hidden state which is variable the last part is funciton of alpha which we define as funciton of hidden state since\n",
    "x is fixed. \n",
    "<img src=\"images/hmm1_12.png\">\n",
    "lets look at the recursion which inclues the emission and transition probabilities and the recursion variable. \n",
    "These 2 equations allow us to calculate alpha1, ..alphan. There are 2 parts to alpha1, the initial distribution p(z1) and hte \n",
    "emission prob. the first one is the initialization step. \n",
    "<img src=\"images/hmm1_13.png\">\n",
    "how much time does the forward algo requrei? compute sum for givne zk. teh multiplicaton the order is sum. O(m) for each \n",
    "zk. how many zk are thre? there are M. O(m^2) for each k. we do for n O(nm^2)\n",
    "<img src=\"images/hmm1_14.png\">\n",
    "forward \n",
    "<img src=\"images/hmm1_15.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLE technique for estimating a parameter from a distribution. Assume a set of distributions, given a data x1..xn. \n",
    "Assume a probabilistic model for our data. Assume D is a ssample from set of RV, X1,,Xn. Goal in MLE is to choose\n",
    "or estimate value of theta that the data comes from. \n",
    "<img src=\"images/mle1.png\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
